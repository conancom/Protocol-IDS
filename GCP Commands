gcloud dataproc jobs submit spark  --jar=gs://example-bucket-visor/testingsbt_2.12-0.1.0-SNAPSHOT.jar     --cluster=cluster-81be  --properties spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2    --region=asia-southeast2


gcloud dataproc jobs submit pyspark     gs://example-bucket-visor/testingsbt_2.12-0.1.0-SNAPSHOT.jar     --cluster=cluster-81be  --properties spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2    --region=asia-southeast2

For Testing to use Spark Streaming:

1) Using 2 properties at once
gcloud dataproc jobs submit spark  --jar=gs://example-bucket-visor/testingsbt_2.12-0.1.0-SNAPSHOT.jar     --cluster=cluster-81be  --properties spark.jars.packages=org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.2 spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2    --region=asia-southeast2

2) Using the one that is missing
gcloud dataproc jobs submit spark  --jar=gs://example-bucket-visor/testingsbt_2.12-0.1.0-SNAPSHOT.jar     --cluster=cluster-81be  --properties spark.jars.packages=org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.2    --region=asia-southeast2

Without Added Libray:

gcloud dataproc jobs submit spark  --jar=gs://example-bucket-visor/testingsbt_2.12-0.1.0-SNAPSHOT.jar     --cluster=cluster-81be  --region=asia-southeast2

Template for Logs:

GET) 
ip:127.0.0.1 user-identifier frank [10/Oct/2000:13:55:36 -0700] header:"GET /apache_pb.gif HTTP/1.0" 200 2326

GET for SQLi) 
ip:127.0.0.1 user-identifier frank [10/Oct/2000:13:55:36 -0700] header:"GET /?id=someting&password=something2 HTTP/1.0" 200 2326

POST) 
ip:127.0.0.1 user-identifier frank [10/Oct/2000:13:55:36 -0700] header:"GET /apache_pb.gif HTTP/1.0" 200 2326

PUT) 
ip:127.0.0.1 user-identifier frank [10/Oct/2000:13:55:36 -0700] header:"GET /apache_pb.gif HTTP/1.0" 200 2326

DELETE) 
ip:127.0.0.1 user-identifier frank [10/Oct/2000:13:55:36 -0700] header:"GET /apache_pb.gif HTTP/1.0" 200 2326


Version: 

  Scala 2.12.14
  Spark 3.1.2
